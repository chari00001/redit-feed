#!/usr/bin/env python3
"""
Posts tablosundaki t√ºm verilerle model eƒüitimi ve performans raporu
"""
import asyncio
import json
import time
import os
from datetime import datetime
from typing import List, Dict, Any
import numpy as np
import pandas as pd
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity

from app.db import database
from app.enhanced_recommender import EnhancedRecommender
from app.content_analyzer import SmartContentAnalyzer

class ModelTrainer:
    def __init__(self):
        self.recommender = None
        self.posts_data = []
        self.training_stats = {}
        self.performance_metrics = {}
        
    async def load_all_posts(self):
        """Posts tablosundaki t√ºm verileri y√ºkle"""
        print("üìä Posts tablosundan t√ºm veriler y√ºkleniyor...")
        
        # T√ºm postlarƒ± √ßek
        query = """
        SELECT 
            id, user_id, title, content, tags, 
            likes_count, comments_count, shares_count, views_count,
            created_at, visibility
        FROM posts 
        ORDER BY created_at DESC
        """
        
        posts = await database.fetch_all(query)
        self.posts_data = [dict(row) for row in posts]
        
        # Etiketleri d√ºzenle
        for post in self.posts_data:
            if isinstance(post.get("tags"), str):
                try:
                    post["tags"] = json.loads(post["tags"])
                except:
                    post["tags"] = []
            elif post.get("tags") is None:
                post["tags"] = []
                
        print(f"‚úÖ {len(self.posts_data)} post y√ºklendi")
        return self.posts_data
    
    def analyze_data_quality(self):
        """Veri kalitesi analizi"""
        print("\nüîç VERƒ∞ KALƒ∞TESƒ∞ ANALƒ∞Zƒ∞")
        print("-" * 50)
        
        total_posts = len(self.posts_data)
        
        # Temel istatistikler
        posts_with_content = sum(1 for p in self.posts_data if p.get('content'))
        posts_with_tags = sum(1 for p in self.posts_data if p.get('tags'))
        posts_with_title = sum(1 for p in self.posts_data if p.get('title'))
        
        # Etkile≈üim istatistikleri
        total_likes = sum(p.get('likes_count', 0) for p in self.posts_data)
        total_comments = sum(p.get('comments_count', 0) for p in self.posts_data)
        total_shares = sum(p.get('shares_count', 0) for p in self.posts_data)
        total_views = sum(p.get('views_count', 0) for p in self.posts_data)
        
        # Tag analizi
        all_tags = []
        for post in self.posts_data:
            if post.get('tags'):
                all_tags.extend(post['tags'])
        
        unique_tags = len(set(all_tags))
        avg_tags_per_post = len(all_tags) / total_posts if total_posts > 0 else 0
        
        stats = {
            'total_posts': total_posts,
            'posts_with_content': posts_with_content,
            'posts_with_tags': posts_with_tags,
            'posts_with_title': posts_with_title,
            'content_coverage': posts_with_content / total_posts * 100,
            'tag_coverage': posts_with_tags / total_posts * 100,
            'title_coverage': posts_with_title / total_posts * 100,
            'total_interactions': total_likes + total_comments + total_shares + total_views,
            'total_likes': total_likes,
            'total_comments': total_comments,
            'total_shares': total_shares,
            'total_views': total_views,
            'unique_tags': unique_tags,
            'avg_tags_per_post': avg_tags_per_post,
            'total_tags_used': len(all_tags)
        }
        
        print(f"üìä Toplam post sayƒ±sƒ±: {stats['total_posts']}")
        print(f"üìù ƒ∞√ßerik kapsamƒ±: {stats['content_coverage']:.1f}% ({stats['posts_with_content']} post)")
        print(f"üè∑Ô∏è Etiket kapsamƒ±: {stats['tag_coverage']:.1f}% ({stats['posts_with_tags']} post)")
        print(f"üìã Ba≈ülƒ±k kapsamƒ±: {stats['title_coverage']:.1f}% ({stats['posts_with_title']} post)")
        print(f"üëç Toplam etkile≈üim: {stats['total_interactions']:,}")
        print(f"üè∑Ô∏è Benzersiz etiket sayƒ±sƒ±: {stats['unique_tags']}")
        print(f"üìä Post ba≈üƒ±na ortalama etiket: {stats['avg_tags_per_post']:.2f}")
        
        self.training_stats['data_quality'] = stats
        return stats
    
    async def train_model(self):
        """Geli≈ümi≈ü model eƒüitimi"""
        print("\nüöÄ MODEL Eƒûƒ∞Tƒ∞Mƒ∞ BA≈ûLATIYOR")
        print("-" * 50)
        
        start_time = time.time()
        
        # Model olu≈ütur ve eƒüit
        self.recommender = EnhancedRecommender()
        
        # ƒ∞√ßerik analizi ile eƒüitim
        print("üî¨ ƒ∞√ßerik analizi ve makine √∂ƒürenmesi modeli eƒüitiliyor...")
        await self.recommender.fit(self.posts_data, use_content_analysis=True)
        
        training_time = time.time() - start_time
        
        print(f"‚úÖ Model eƒüitimi tamamlandƒ± ({training_time:.2f} saniye)")
        print(f"üéØ √ñzellik boyutu: {self.recommender.feature_matrix.shape}")
        
        self.training_stats['training_time'] = training_time
        self.training_stats['feature_dimensions'] = self.recommender.feature_matrix.shape
        
        return self.recommender
    
    def evaluate_clustering_performance(self):
        """K√ºmeleme performansƒ±nƒ± deƒüerlendir"""
        print("\nüìä K√úMELEME PERFORMANSI DEƒûERLENDƒ∞RMESƒ∞")
        print("-" * 50)
        
        if not hasattr(self.recommender.content_analyzer, 'cluster_labels'):
            print("‚ùå K√ºmeleme analizi bulunamadƒ±")
            return {}
        
        # K√ºmeleme metrikleri
        labels = self.recommender.content_analyzer.cluster_labels
        feature_matrix = self.recommender.content_analyzer.tfidf_matrix
        
        # Silhouette skoru
        if len(set(labels)) > 1:
            silhouette_avg = silhouette_score(feature_matrix, labels)
            calinski_score = calinski_harabasz_score(feature_matrix.toarray(), labels)
        else:
            silhouette_avg = 0
            calinski_score = 0
        
        # K√ºme istatistikleri
        unique_labels = set(labels)
        cluster_sizes = {label: list(labels).count(label) for label in unique_labels}
        
        clustering_metrics = {
            'n_clusters': len(unique_labels),
            'silhouette_score': silhouette_avg,
            'calinski_harabasz_score': calinski_score,
            'cluster_sizes': cluster_sizes,
            'largest_cluster_size': max(cluster_sizes.values()) if cluster_sizes else 0,
            'smallest_cluster_size': min(cluster_sizes.values()) if cluster_sizes else 0,
            'avg_cluster_size': np.mean(list(cluster_sizes.values())) if cluster_sizes else 0
        }
        
        print(f"üéØ K√ºme sayƒ±sƒ±: {clustering_metrics['n_clusters']}")
        print(f"üìä Silhouette skoru: {clustering_metrics['silhouette_score']:.4f}")
        print(f"üìà Calinski-Harabasz skoru: {clustering_metrics['calinski_harabasz_score']:.2f}")
        print(f"üìè Ortalama k√ºme boyutu: {clustering_metrics['avg_cluster_size']:.1f}")
        print(f"üì¶ En b√ºy√ºk k√ºme: {clustering_metrics['largest_cluster_size']} post")
        print(f"üì¶ En k√º√ß√ºk k√ºme: {clustering_metrics['smallest_cluster_size']} post")
        
        self.performance_metrics['clustering'] = clustering_metrics
        return clustering_metrics
    
    async def evaluate_recommendation_quality(self):
        """√ñneri kalitesini deƒüerlendir"""
        print("\nüéØ √ñNERƒ∞ KALƒ∞TESƒ∞ DEƒûERLENDƒ∞RMESƒ∞")
        print("-" * 50)
        
        # Test kullanƒ±cƒ±larƒ± i√ßin √∂neri kalitesini deƒüerlendir
        test_users = [1, 5, 10, 25, 34, 50] 
        all_recommendations = []
        
        print("√ñrnek kullanƒ±cƒ±lar i√ßin √∂neriler olu≈üturuluyor:")
        for user_id in test_users:
            try:
                # √ñnerileri al
                recommendations = await self.recommender.recommend_for_user(user_id, top_n=10)
                
                if recommendations:
                    all_recommendations.extend(recommendations)
                    print(f"  ‚úÖ Kullanƒ±cƒ± {user_id}: {len(recommendations)} √∂neri bulundu.")
                else:
                    print(f"  ‚ö†Ô∏è Kullanƒ±cƒ± {user_id}: √ñneri bulunamadƒ± veya profil yetersiz.")

                # Profilini al ve yazdƒ±r (hata ayƒ±klama i√ßin faydalƒ±)
                user_profile = await self.recommender.get_user_interest_profile(user_id)
                if user_profile and user_profile.get('status') == 'profile_found':
                    interests = user_profile.get('top_interests', [])
                    print(f"    -> Profil: {interests[:3]}")

            except Exception as e:
                print(f"  ‚ùå Kullanƒ±cƒ± {user_id} i√ßin √∂neri alƒ±nƒ±rken hata olu≈ütu: {e}")
        
        print("-" * 50)

        # √ñneri kalitesi metrikleri
        if all_recommendations:
            quality_metrics = {
                'avg_recommendation_score': np.mean([rec.get('recommendation_score', 0) for rec in all_recommendations]),
                'max_recommendation_score': np.max([rec.get('recommendation_score', 0) for rec in all_recommendations]),
                'min_recommendation_score': np.min([rec.get('recommendation_score', 0) for rec in all_recommendations]),
                'std_recommendation_score': np.std([rec.get('recommendation_score', 0) for rec in all_recommendations]),
                'total_recommendations_generated': len(all_recommendations)
            }
        else:
            quality_metrics = {
                'avg_recommendation_score': 0,
                'max_recommendation_score': 0,
                'min_recommendation_score': 0,
                'std_recommendation_score': 0,
                'total_recommendations_generated': 0
            }
        
        print(f"üìä Ortalama √∂neri skoru: {quality_metrics['avg_recommendation_score']:.4f}")
        print(f"üìà Maksimum √∂neri skoru: {quality_metrics['max_recommendation_score']:.4f}")
        print(f"üìâ Minimum √∂neri skoru: {quality_metrics['min_recommendation_score']:.4f}")
        print(f"üìè Skor standart sapmasƒ±: {quality_metrics['std_recommendation_score']:.4f}")
        print(f"üéØ Toplam √∂neri sayƒ±sƒ±: {quality_metrics['total_recommendations_generated']}")
        
        self.performance_metrics['recommendation_quality'] = quality_metrics
        return quality_metrics
    
    def evaluate_tag_analysis(self):
        """Tag analizi performansƒ±nƒ± deƒüerlendir"""
        print("\nüè∑Ô∏è ETƒ∞KET ANALƒ∞Zƒ∞ PERFORMANSI")
        print("-" * 50)
        
        # Orijinal vs enhanced tag kar≈üƒ±la≈ütƒ±rmasƒ±
        original_tag_count = 0
        enhanced_tag_count = 0
        posts_with_enhanced_tags = 0
        
        for post in self.posts_data:
            original_tags = post.get('tags', [])
            original_tag_count += len(original_tags)
            
            if post['id'] in self.recommender.post_analysis:
                enhanced_tags = self.recommender.post_analysis[post['id']].get('enhanced_tags', [])
                enhanced_tag_count += len(enhanced_tags)
                if len(enhanced_tags) > len(original_tags):
                    posts_with_enhanced_tags += 1
        
        tag_metrics = {
            'original_total_tags': original_tag_count,
            'enhanced_total_tags': enhanced_tag_count,
            'tag_enhancement_ratio': enhanced_tag_count / max(original_tag_count, 1),
            'posts_with_enhanced_tags': posts_with_enhanced_tags,
            'enhancement_coverage': posts_with_enhanced_tags / len(self.posts_data) * 100
        }
        
        print(f"üìä Orijinal toplam etiket: {tag_metrics['original_total_tags']}")
        print(f"üöÄ Geli≈ütirilmi≈ü toplam etiket: {tag_metrics['enhanced_total_tags']}")
        print(f"üìà Etiket artƒ±≈ü oranƒ±: {tag_metrics['tag_enhancement_ratio']:.2f}x")
        print(f"üì¶ Geli≈ütirilen post sayƒ±sƒ±: {tag_metrics['posts_with_enhanced_tags']}")
        print(f"üìä Geli≈ütirme kapsamƒ±: {tag_metrics['enhancement_coverage']:.1f}%")
        
        self.performance_metrics['tag_analysis'] = tag_metrics
        return tag_metrics
    
    def generate_topic_analysis(self):
        """Konu analizi raporu"""
        print("\nüéØ KONU ANALƒ∞Zƒ∞ RAPORU")
        print("-" * 50)
        
        topics_summary = self.recommender.get_topics_summary()
        
        if topics_summary['topics']:
            print(f"üìä Toplam konu sayƒ±sƒ±: {topics_summary['total_topics']}")
            print(f"üìù Analiz edilen post sayƒ±sƒ±: {topics_summary['total_posts_analyzed']}")
            
            print("\nüèÜ EN POP√úLER KONULAR:")
            for i, topic in enumerate(topics_summary['topics'][:5]):
                keywords_str = ', '.join(topic['keywords'][:5])
                print(f"   {i+1}. Konu {topic['topic_id']}: {keywords_str}")
                print(f"      üìä Post sayƒ±sƒ±: {topic['post_count']}")
        
        self.performance_metrics['topic_analysis'] = topics_summary
        return topics_summary
    
    def save_model_and_report(self):
        """Model ve raporu kaydet"""
        print("\nüíæ MODEL VE RAPOR KAYDETME")
        print("-" * 50)
        
        # Model klas√∂r√º olu≈ütur
        os.makedirs("trained_models", exist_ok=True)
        os.makedirs("reports", exist_ok=True)
        
        # Modeli kaydet
        self.recommender.save_models("trained_models/")
        
        # Performans raporunu olu≈ütur
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        full_report = {
            'timestamp': timestamp,
            'training_stats': self.training_stats,
            'performance_metrics': self.performance_metrics,
            'model_info': {
                'total_posts_trained': len(self.posts_data),
                'feature_matrix_shape': list(self.recommender.feature_matrix.shape),
                'content_analysis_enabled': True,
                'model_type': 'EnhancedRecommender with SmartContentAnalyzer'
            }
        }
        
        # JSON raporu kaydet
        report_file = f"reports/performance_report_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(full_report, f, indent=2, ensure_ascii=False, default=str)
        
        # Markdown raporu olu≈ütur
        markdown_report = self.generate_markdown_report(full_report)
        markdown_file = f"reports/performance_report_{timestamp}.md"
        with open(markdown_file, 'w', encoding='utf-8') as f:
            f.write(markdown_report)
        
        print(f"‚úÖ Model 'trained_models/' klas√∂r√ºne kaydedildi")
        print(f"üìä JSON rapor: {report_file}")
        print(f"üìù Markdown rapor: {markdown_file}")
        
        return full_report
    
    def generate_markdown_report(self, report_data):
        """Markdown formatƒ±nda rapor olu≈ütur"""
        timestamp = report_data['timestamp']
        
        md_content = f"""# Model Eƒüitimi ve Performans Raporu
## {timestamp}

## üìä Genel Bilgiler
- **Eƒüitim Tarihi**: {timestamp}
- **Toplam Post Sayƒ±sƒ±**: {report_data['model_info']['total_posts_trained']:,}
- **√ñzellik Matrisi Boyutu**: {report_data['model_info']['feature_matrix_shape']}
- **Model T√ºr√º**: {report_data['model_info']['model_type']}
- **Eƒüitim S√ºresi**: {report_data['training_stats'].get('training_time', 0):.2f} saniye

## üîç Veri Kalitesi Analizi
"""
        
        if 'data_quality' in report_data['training_stats']:
            dq = report_data['training_stats']['data_quality']
            md_content += f"""
- **Toplam Post**: {dq['total_posts']:,}
- **ƒ∞√ßerik Kapsamƒ±**: {dq['content_coverage']:.1f}% ({dq['posts_with_content']:} post)
- **Etiket Kapsamƒ±**: {dq['tag_coverage']:.1f}% ({dq['posts_with_tags']:} post)
- **Ba≈ülƒ±k Kapsamƒ±**: {dq['title_coverage']:.1f}% ({dq['posts_with_title']:} post)
- **Toplam Etkile≈üim**: {dq['total_interactions']:}
- **Benzersiz Etiket**: {dq['unique_tags']:}
- **Ortalama Etiket/Post**: {dq['avg_tags_per_post']:.2f}
"""
        
        if 'clustering' in report_data['performance_metrics']:
            cl = report_data['performance_metrics']['clustering']
            md_content += f"""
## üéØ K√ºmeleme Performansƒ±
- **K√ºme Sayƒ±sƒ±**: {cl['n_clusters']}
- **Silhouette Skoru**: {cl['silhouette_score']:.4f}
- **Calinski-Harabasz Skoru**: {cl['calinski_harabasz_score']:.2f}
- **Ortalama K√ºme Boyutu**: {cl['avg_cluster_size']:.1f}
- **En B√ºy√ºk K√ºme**: {cl['largest_cluster_size']} post
- **En K√º√ß√ºk K√ºme**: {cl['smallest_cluster_size']} post
"""
        
        if 'recommendation_quality' in report_data['performance_metrics']:
            rq = report_data['performance_metrics']['recommendation_quality']
            md_content += f"""
## üéØ √ñneri Sistemi Performansƒ±
- **Ortalama √ñneri Skoru**: {rq['avg_recommendation_score']:.4f}
- **Maksimum √ñneri Skoru**: {rq['max_recommendation_score']:.4f}
- **Minimum √ñneri Skoru**: {rq['min_recommendation_score']:.4f}
- **Skor Standart Sapmasƒ±**: {rq['std_recommendation_score']:.4f}
- **Toplam √ñneri Sayƒ±sƒ±**: {rq['total_recommendations_generated']}
"""
        
        if 'tag_analysis' in report_data['performance_metrics']:
            ta = report_data['performance_metrics']['tag_analysis']
            md_content += f"""
## üè∑Ô∏è Etiket Analizi Performansƒ±
- **Orijinal Toplam Etiket**: {ta['original_total_tags']:}
- **Geli≈ütirilmi≈ü Toplam Etiket**: {ta['enhanced_total_tags']:}
- **Etiket Artƒ±≈ü Oranƒ±**: {ta['tag_enhancement_ratio']:.2f}x
- **Geli≈ütirilen Post Sayƒ±sƒ±**: {ta['posts_with_enhanced_tags']:}
- **Geli≈ütirme Kapsamƒ±**: {ta['enhancement_coverage']:.1f}%
"""
        
        if 'topic_analysis' in report_data['performance_metrics']:
            ta = report_data['performance_metrics']['topic_analysis']
            md_content += f"""
## üéØ Konu Analizi
- **Toplam Konu Sayƒ±sƒ±**: {ta['total_topics']}
- **Analiz Edilen Post**: {ta['total_posts_analyzed']:}

### En Pop√ºler Konular
"""
            for i, topic in enumerate(ta['topics'][:5]):
                keywords = ', '.join(topic['keywords'][:5])
                md_content += f"{i+1}. **Konu {topic['topic_id']}**: {keywords} ({topic['post_count']} post)\n"
        
        md_content += f"""
## üìà √ñzet ve √ñneriler

### Ba≈üarƒ± Metrikleri
‚úÖ Model ba≈üarƒ±yla eƒüitildi ve test edildi
‚úÖ T√ºm Posts tablosu verisi kullanƒ±ldƒ±
‚úÖ ƒ∞√ßerik analizi ve k√ºmeleme tamamlandƒ±
‚úÖ Ki≈üiselle≈ütirilmi≈ü √∂neri sistemi aktif

### Performans Deƒüerlendirmesi
Model performansƒ± yukarƒ±daki metrikler doƒürultusunda deƒüerlendirilmi≈ütir. 
Sistem ger√ßek zamanlƒ± √∂neriler sunmaya hazƒ±rdƒ±r.

---
*Rapor {timestamp} tarihinde otomatik olarak olu≈üturulmu≈ütur.*
"""
        
        return md_content

async def main():
    """Ana fonksiyon"""
    print("üöÄ POSTS TABLOSU ƒ∞LE MODEL Eƒûƒ∞Tƒ∞Mƒ∞")
    print("=" * 60)
    
    trainer = ModelTrainer()
    
    try:
        # Veritabanƒ±na baƒülan
        await database.connect()
        print("‚úÖ Veritabanƒ± baƒülantƒ±sƒ± kuruldu")
        
        # Veri y√ºkle ve kaliteyi analiz et
        await trainer.load_all_posts()
        trainer.analyze_data_quality()
        
        # Modeli eƒüit
        await trainer.train_model()
        
        # Performansƒ± deƒüerlendir
        trainer.evaluate_clustering_performance()
        await trainer.evaluate_recommendation_quality()
        trainer.evaluate_tag_analysis()
        trainer.generate_topic_analysis()
        
        # Raporlarƒ± olu≈ütur ve kaydet
        trainer.save_model_and_report()

        print("\nüéâ MODEL Eƒûƒ∞Tƒ∞Mƒ∞ VE PERFORMANS ANALƒ∞Zƒ∞ TAMAMLANDI!")
        print("=" * 60)
        print(f"üìä Toplam i≈ülenen post: {len(trainer.posts_data):,}")
        print(f"‚ö° Eƒüitim s√ºresi: {trainer.training_stats.get('training_time', 0):.2f} saniye")
        print("üìÅ Sonu√ßlar 'trained_models/' ve 'reports/' klas√∂rlerinde")

    except Exception as e:
        print(f"‚ùå Hata olu≈ütu: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if database.is_connected:
            await database.disconnect()
            print("‚úÖ Veritabanƒ± baƒülantƒ±sƒ± kapatƒ±ldƒ±")

if __name__ == "__main__":
    asyncio.run(main()) 