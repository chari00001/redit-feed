#!/usr/bin/env python3
"""
Posts tablosundaki tÃ¼m verilerle model eÄŸitimi ve performans raporu
"""
import asyncio
import json
import time
import os
from datetime import datetime
from typing import List, Dict, Any
import numpy as np
import pandas as pd
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity

from app.db import database
from app.enhanced_recommender import EnhancedRecommender
from app.content_analyzer import SmartContentAnalyzer

class ModelTrainer:
    def __init__(self):
        self.recommender = None
        self.posts_data = []
        self.training_stats = {}
        self.performance_metrics = {}
        
    async def load_all_posts(self):
        """Posts tablosundaki tÃ¼m verileri yÃ¼kle"""
        print("ğŸ“Š Posts tablosundan tÃ¼m veriler yÃ¼kleniyor...")
        
        # TÃ¼m postlarÄ± Ã§ek
        query = """
        SELECT 
            id, user_id, title, content, tags, 
            likes_count, comments_count, shares_count, views_count,
            created_at, visibility
        FROM posts 
        ORDER BY created_at DESC
        """
        
        posts = await database.fetch_all(query)
        self.posts_data = [dict(row) for row in posts]
        
        # Etiketleri dÃ¼zenle
        for post in self.posts_data:
            if isinstance(post.get("tags"), str):
                try:
                    post["tags"] = json.loads(post["tags"])
                except:
                    post["tags"] = []
            elif post.get("tags") is None:
                post["tags"] = []
                
        print(f"âœ… {len(self.posts_data)} post yÃ¼klendi")
        return self.posts_data
    
    def analyze_data_quality(self):
        """Veri kalitesi analizi"""
        print("\nğŸ” VERÄ° KALÄ°TESÄ° ANALÄ°ZÄ°")
        print("-" * 50)
        
        total_posts = len(self.posts_data)
        
        # Temel istatistikler
        posts_with_content = sum(1 for p in self.posts_data if p.get('content'))
        posts_with_tags = sum(1 for p in self.posts_data if p.get('tags'))
        posts_with_title = sum(1 for p in self.posts_data if p.get('title'))
        
        # EtkileÅŸim istatistikleri
        total_likes = sum(p.get('likes_count', 0) for p in self.posts_data)
        total_comments = sum(p.get('comments_count', 0) for p in self.posts_data)
        total_shares = sum(p.get('shares_count', 0) for p in self.posts_data)
        total_views = sum(p.get('views_count', 0) for p in self.posts_data)
        
        # Tag analizi
        all_tags = []
        for post in self.posts_data:
            if post.get('tags'):
                all_tags.extend(post['tags'])
        
        unique_tags = len(set(all_tags))
        avg_tags_per_post = len(all_tags) / total_posts if total_posts > 0 else 0
        
        stats = {
            'total_posts': total_posts,
            'posts_with_content': posts_with_content,
            'posts_with_tags': posts_with_tags,
            'posts_with_title': posts_with_title,
            'content_coverage': posts_with_content / total_posts * 100,
            'tag_coverage': posts_with_tags / total_posts * 100,
            'title_coverage': posts_with_title / total_posts * 100,
            'total_interactions': total_likes + total_comments + total_shares + total_views,
            'total_likes': total_likes,
            'total_comments': total_comments,
            'total_shares': total_shares,
            'total_views': total_views,
            'unique_tags': unique_tags,
            'avg_tags_per_post': avg_tags_per_post,
            'total_tags_used': len(all_tags)
        }
        
        print(f"ğŸ“Š Toplam post sayÄ±sÄ±: {stats['total_posts']}")
        print(f"ğŸ“ Ä°Ã§erik kapsamÄ±: {stats['content_coverage']:.1f}% ({stats['posts_with_content']} post)")
        print(f"ğŸ·ï¸ Etiket kapsamÄ±: {stats['tag_coverage']:.1f}% ({stats['posts_with_tags']} post)")
        print(f"ğŸ“‹ BaÅŸlÄ±k kapsamÄ±: {stats['title_coverage']:.1f}% ({stats['posts_with_title']} post)")
        print(f"ğŸ‘ Toplam etkileÅŸim: {stats['total_interactions']:,}")
        print(f"ğŸ·ï¸ Benzersiz etiket sayÄ±sÄ±: {stats['unique_tags']}")
        print(f"ğŸ“Š Post baÅŸÄ±na ortalama etiket: {stats['avg_tags_per_post']:.2f}")
        
        self.training_stats['data_quality'] = stats
        return stats
    
    async def train_model(self):
        """GeliÅŸmiÅŸ model eÄŸitimi"""
        print("\nğŸš€ MODEL EÄÄ°TÄ°MÄ° BAÅLATIYOR")
        print("-" * 50)
        
        start_time = time.time()
        
        # Model oluÅŸtur ve eÄŸit
        self.recommender = EnhancedRecommender()
        
        # Ä°Ã§erik analizi ile eÄŸitim
        print("ğŸ”¬ Ä°Ã§erik analizi ve makine Ã¶ÄŸrenmesi modeli eÄŸitiliyor...")
        await self.recommender.fit(self.posts_data, use_content_analysis=True)
        
        training_time = time.time() - start_time
        
        print(f"âœ… Model eÄŸitimi tamamlandÄ± ({training_time:.2f} saniye)")
        print(f"ğŸ¯ Ã–zellik boyutu: {self.recommender.feature_matrix.shape}")
        
        self.training_stats['training_time'] = training_time
        self.training_stats['feature_dimensions'] = self.recommender.feature_matrix.shape
        
        return self.recommender
    
    def evaluate_clustering_performance(self):
        """KÃ¼meleme performansÄ±nÄ± deÄŸerlendir"""
        print("\nğŸ“Š KÃœMELEME PERFORMANSI DEÄERLENDÄ°RMESÄ°")
        print("-" * 50)
        
        if not hasattr(self.recommender.content_analyzer, 'cluster_labels'):
            print("âŒ KÃ¼meleme analizi bulunamadÄ±")
            return {}
        
        # KÃ¼meleme metrikleri
        labels = self.recommender.content_analyzer.cluster_labels
        feature_matrix = self.recommender.content_analyzer.tfidf_matrix
        
        # Silhouette skoru
        if len(set(labels)) > 1:
            silhouette_avg = silhouette_score(feature_matrix, labels)
            calinski_score = calinski_harabasz_score(feature_matrix.toarray(), labels)
        else:
            silhouette_avg = 0
            calinski_score = 0
        
        # KÃ¼me istatistikleri
        unique_labels = set(labels)
        cluster_sizes = {label: list(labels).count(label) for label in unique_labels}
        
        clustering_metrics = {
            'n_clusters': len(unique_labels),
            'silhouette_score': silhouette_avg,
            'calinski_harabasz_score': calinski_score,
            'cluster_sizes': cluster_sizes,
            'largest_cluster_size': max(cluster_sizes.values()) if cluster_sizes else 0,
            'smallest_cluster_size': min(cluster_sizes.values()) if cluster_sizes else 0,
            'avg_cluster_size': np.mean(list(cluster_sizes.values())) if cluster_sizes else 0
        }
        
        print(f"ğŸ¯ KÃ¼me sayÄ±sÄ±: {clustering_metrics['n_clusters']}")
        print(f"ğŸ“Š Silhouette skoru: {clustering_metrics['silhouette_score']:.4f}")
        print(f"ğŸ“ˆ Calinski-Harabasz skoru: {clustering_metrics['calinski_harabasz_score']:.2f}")
        print(f"ğŸ“ Ortalama kÃ¼me boyutu: {clustering_metrics['avg_cluster_size']:.1f}")
        print(f"ğŸ“¦ En bÃ¼yÃ¼k kÃ¼me: {clustering_metrics['largest_cluster_size']} post")
        print(f"ğŸ“¦ En kÃ¼Ã§Ã¼k kÃ¼me: {clustering_metrics['smallest_cluster_size']} post")
        
        self.performance_metrics['clustering'] = clustering_metrics
        return clustering_metrics
    
    async def evaluate_recommendation_quality(self):
        """Ã–neri kalitesini deÄŸerlendir"""
        print("\nğŸ¯ Ã–NERÄ° KALÄ°TESÄ° DEÄERLENDÄ°RMESÄ°")
        print("-" * 50)
        
        # Test kullanÄ±cÄ±larÄ± iÃ§in Ã¶neri kalitesini deÄŸerlendir
        test_users = [1, 5, 10, 25, 34, 50] 
        all_recommendations = []
        
        print("Ã–rnek kullanÄ±cÄ±lar iÃ§in Ã¶neriler oluÅŸturuluyor:")
        for user_id in test_users:
            try:
                # Ã–nerileri al
                recommendations = await self.recommender.recommend_for_user(user_id, top_n=10)
                
                if recommendations:
                    all_recommendations.extend(recommendations)
                    print(f"  âœ… KullanÄ±cÄ± {user_id}: {len(recommendations)} Ã¶neri bulundu.")
                else:
                    print(f"  âš ï¸ KullanÄ±cÄ± {user_id}: Ã–neri bulunamadÄ± veya profil yetersiz.")

                # Profilini al ve yazdÄ±r (hata ayÄ±klama iÃ§in faydalÄ±)
                user_profile = await self.recommender.get_user_interest_profile(user_id)
                if user_profile and user_profile.get('status') == 'profile_found':
                    interests = user_profile.get('top_interests', [])
                    print(f"    -> Profil: {interests[:3]}")

            except Exception as e:
                print(f"  âŒ KullanÄ±cÄ± {user_id} iÃ§in Ã¶neri alÄ±nÄ±rken hata oluÅŸtu: {e}")
        
        print("-" * 50)

        # Ã–neri kalitesi metrikleri
        if all_recommendations:
            quality_metrics = {
                'avg_recommendation_score': np.mean([rec.get('recommendation_score', 0) for rec in all_recommendations]),
                'max_recommendation_score': np.max([rec.get('recommendation_score', 0) for rec in all_recommendations]),
                'min_recommendation_score': np.min([rec.get('recommendation_score', 0) for rec in all_recommendations]),
                'std_recommendation_score': np.std([rec.get('recommendation_score', 0) for rec in all_recommendations]),
                'total_recommendations_generated': len(all_recommendations)
            }
        else:
            quality_metrics = {
                'avg_recommendation_score': 0,
                'max_recommendation_score': 0,
                'min_recommendation_score': 0,
                'std_recommendation_score': 0,
                'total_recommendations_generated': 0
            }
        
        print(f"ğŸ“Š Ortalama Ã¶neri skoru: {quality_metrics['avg_recommendation_score']:.4f}")
        print(f"ğŸ“ˆ Maksimum Ã¶neri skoru: {quality_metrics['max_recommendation_score']:.4f}")
        print(f"ğŸ“‰ Minimum Ã¶neri skoru: {quality_metrics['min_recommendation_score']:.4f}")
        print(f"ğŸ“ Skor standart sapmasÄ±: {quality_metrics['std_recommendation_score']:.4f}")
        print(f"ğŸ¯ Toplam Ã¶neri sayÄ±sÄ±: {quality_metrics['total_recommendations_generated']}")
        
        self.performance_metrics['recommendation_quality'] = quality_metrics
        return quality_metrics
    
    def evaluate_tag_analysis(self):
        """Tag analizi performansÄ±nÄ± deÄŸerlendir"""
        print("\nğŸ·ï¸ ETÄ°KET ANALÄ°ZÄ° PERFORMANSI")
        print("-" * 50)
        
        # Orijinal vs enhanced tag karÅŸÄ±laÅŸtÄ±rmasÄ±
        original_tag_count = 0
        enhanced_tag_count = 0
        posts_with_enhanced_tags = 0
        
        for post in self.posts_data:
            original_tags = post.get('tags', [])
            original_tag_count += len(original_tags)
            
            if post['id'] in self.recommender.post_analysis:
                enhanced_tags = self.recommender.post_analysis[post['id']].get('enhanced_tags', [])
                enhanced_tag_count += len(enhanced_tags)
                if len(enhanced_tags) > len(original_tags):
                    posts_with_enhanced_tags += 1
        
        tag_metrics = {
            'original_total_tags': original_tag_count,
            'enhanced_total_tags': enhanced_tag_count,
            'tag_enhancement_ratio': enhanced_tag_count / max(original_tag_count, 1),
            'posts_with_enhanced_tags': posts_with_enhanced_tags,
            'enhancement_coverage': posts_with_enhanced_tags / len(self.posts_data) * 100
        }
        
        print(f"ğŸ“Š Orijinal toplam etiket: {tag_metrics['original_total_tags']}")
        print(f"ğŸš€ GeliÅŸtirilmiÅŸ toplam etiket: {tag_metrics['enhanced_total_tags']}")
        print(f"ğŸ“ˆ Etiket artÄ±ÅŸ oranÄ±: {tag_metrics['tag_enhancement_ratio']:.2f}x")
        print(f"ğŸ“¦ GeliÅŸtirilen post sayÄ±sÄ±: {tag_metrics['posts_with_enhanced_tags']}")
        print(f"ğŸ“Š GeliÅŸtirme kapsamÄ±: {tag_metrics['enhancement_coverage']:.1f}%")
        
        self.performance_metrics['tag_analysis'] = tag_metrics
        return tag_metrics
    
    def generate_topic_analysis(self):
        """Konu analizi raporu"""
        print("\nğŸ¯ KONU ANALÄ°ZÄ° RAPORU")
        print("-" * 50)
        
        topics_summary = self.recommender.get_topics_summary()
        
        if topics_summary['topics']:
            print(f"ğŸ“Š Toplam konu sayÄ±sÄ±: {topics_summary['total_topics']}")
            print(f"ğŸ“ Analiz edilen post sayÄ±sÄ±: {topics_summary['total_posts_analyzed']}")
            
            print("\nğŸ† EN POPÃœLER KONULAR:")
            for i, topic in enumerate(topics_summary['topics'][:5]):
                keywords_str = ', '.join(topic['keywords'][:5])
                print(f"   {i+1}. Konu {topic['topic_id']}: {keywords_str}")
                print(f"      ğŸ“Š Post sayÄ±sÄ±: {topic['post_count']}")
        
        self.performance_metrics['topic_analysis'] = topics_summary
        return topics_summary
    
    def save_model_and_report(self):
        """Model ve raporu kaydet"""
        print("\nğŸ’¾ MODEL VE RAPOR KAYDETME")
        print("-" * 50)
        
        # Model klasÃ¶rÃ¼ oluÅŸtur
        os.makedirs("trained_models", exist_ok=True)
        os.makedirs("reports", exist_ok=True)
        
        # Modeli kaydet
        self.recommender.save_models("trained_models/")
        
        # Performans raporunu oluÅŸtur
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        full_report = {
            'timestamp': timestamp,
            'training_stats': self.training_stats,
            'performance_metrics': self.performance_metrics,
            'model_info': {
                'total_posts_trained': len(self.posts_data),
                'feature_matrix_shape': list(self.recommender.feature_matrix.shape),
                'content_analysis_enabled': True,
                'model_type': 'EnhancedRecommender with SmartContentAnalyzer'
            }
        }
        
        # JSON raporu kaydet
        report_file = f"reports/performance_report_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(full_report, f, indent=2, ensure_ascii=False, default=str)
        
        # Markdown raporu oluÅŸtur
        markdown_report = self.generate_markdown_report(full_report)
        markdown_file = f"reports/performance_report_{timestamp}.md"
        with open(markdown_file, 'w', encoding='utf-8') as f:
            f.write(markdown_report)
        
        print(f"âœ… Model 'trained_models/' klasÃ¶rÃ¼ne kaydedildi")
        print(f"ğŸ“Š JSON rapor: {report_file}")
        print(f"ğŸ“ Markdown rapor: {markdown_file}")
        
        return full_report
    
    def generate_markdown_report(self, report_data):
        """Markdown formatÄ±nda rapor oluÅŸtur"""
        timestamp = report_data['timestamp']
        
        md_content = f"""# Model EÄŸitimi ve Performans Raporu
## {timestamp}

## ğŸ“Š Genel Bilgiler
- **EÄŸitim Tarihi**: {timestamp}
- **Toplam Post SayÄ±sÄ±**: {report_data['model_info']['total_posts_trained']:,}
- **Ã–zellik Matrisi Boyutu**: {report_data['model_info']['feature_matrix_shape']}
- **Model TÃ¼rÃ¼**: {report_data['model_info']['model_type']}
- **EÄŸitim SÃ¼resi**: {report_data['training_stats'].get('training_time', 0):.2f} saniye

## ğŸ” Veri Kalitesi Analizi
"""
        
        if 'data_quality' in report_data['training_stats']:
            dq = report_data['training_stats']['data_quality']
            md_content += f"""
- **Toplam Post**: {dq['total_posts']:,}
- **Ä°Ã§erik KapsamÄ±**: {dq['content_coverage']:.1f}% ({dq['posts_with_content']:} post)
- **Etiket KapsamÄ±**: {dq['tag_coverage']:.1f}% ({dq['posts_with_tags']:} post)
- **BaÅŸlÄ±k KapsamÄ±**: {dq['title_coverage']:.1f}% ({dq['posts_with_title']:} post)
- **Toplam EtkileÅŸim**: {dq['total_interactions']:}
- **Benzersiz Etiket**: {dq['unique_tags']:}
- **Ortalama Etiket/Post**: {dq['avg_tags_per_post']:.2f}
"""
        
        if 'clustering' in report_data['performance_metrics']:
            cl = report_data['performance_metrics']['clustering']
            md_content += f"""
## ğŸ¯ KÃ¼meleme PerformansÄ±
- **KÃ¼me SayÄ±sÄ±**: {cl['n_clusters']}
- **Silhouette Skoru**: {cl['silhouette_score']:.4f}
- **Calinski-Harabasz Skoru**: {cl['calinski_harabasz_score']:.2f}
- **Ortalama KÃ¼me Boyutu**: {cl['avg_cluster_size']:.1f}
- **En BÃ¼yÃ¼k KÃ¼me**: {cl['largest_cluster_size']} post
- **En KÃ¼Ã§Ã¼k KÃ¼me**: {cl['smallest_cluster_size']} post
"""
        
        if 'recommendation_quality' in report_data['performance_metrics']:
            rq = report_data['performance_metrics']['recommendation_quality']
            md_content += f"""
## ğŸ¯ Ã–neri Sistemi PerformansÄ±
- **Ortalama Ã–neri Skoru**: {rq['avg_recommendation_score']:.4f}
- **Maksimum Ã–neri Skoru**: {rq['max_recommendation_score']:.4f}
- **Minimum Ã–neri Skoru**: {rq['min_recommendation_score']:.4f}
- **Skor Standart SapmasÄ±**: {rq['std_recommendation_score']:.4f}
- **Toplam Ã–neri SayÄ±sÄ±**: {rq['total_recommendations_generated']}
"""
        
        if 'tag_analysis' in report_data['performance_metrics']:
            ta = report_data['performance_metrics']['tag_analysis']
            md_content += f"""
## ğŸ·ï¸ Etiket Analizi PerformansÄ±
- **Orijinal Toplam Etiket**: {ta['original_total_tags']:}
- **GeliÅŸtirilmiÅŸ Toplam Etiket**: {ta['enhanced_total_tags']:}
- **Etiket ArtÄ±ÅŸ OranÄ±**: {ta['tag_enhancement_ratio']:.2f}x
- **GeliÅŸtirilen Post SayÄ±sÄ±**: {ta['posts_with_enhanced_tags']:}
- **GeliÅŸtirme KapsamÄ±**: {ta['enhancement_coverage']:.1f}%
"""
        
        if 'topic_analysis' in report_data['performance_metrics']:
            ta = report_data['performance_metrics']['topic_analysis']
            md_content += f"""
## ğŸ¯ Konu Analizi
- **Toplam Konu SayÄ±sÄ±**: {ta['total_topics']}
- **Analiz Edilen Post**: {ta['total_posts_analyzed']:}

### En PopÃ¼ler Konular
"""
            for i, topic in enumerate(ta['topics'][:5]):
                keywords = ', '.join(topic['keywords'][:5])
                md_content += f"{i+1}. **Konu {topic['topic_id']}**: {keywords} ({topic['post_count']} post)\n"
        
        md_content += f"""
## ğŸ“ˆ Ã–zet ve Ã–neriler

### BaÅŸarÄ± Metrikleri
âœ… Model baÅŸarÄ±yla eÄŸitildi ve test edildi
âœ… TÃ¼m Posts tablosu verisi kullanÄ±ldÄ±
âœ… Ä°Ã§erik analizi ve kÃ¼meleme tamamlandÄ±
âœ… KiÅŸiselleÅŸtirilmiÅŸ Ã¶neri sistemi aktif

### Performans DeÄŸerlendirmesi
Model performansÄ± yukarÄ±daki metrikler doÄŸrultusunda deÄŸerlendirilmiÅŸtir. 
Sistem gerÃ§ek zamanlÄ± Ã¶neriler sunmaya hazÄ±rdÄ±r.

---
*Rapor {timestamp} tarihinde otomatik olarak oluÅŸturulmuÅŸtur.*
"""
        
        return md_content

async def main():
    """Ana fonksiyon"""
    print("ğŸš€ POSTS TABLOSU Ä°LE MODEL EÄÄ°TÄ°MÄ°")
    print("=" * 60)
    
    trainer = ModelTrainer()
    
    try:
        # VeritabanÄ±na baÄŸlan
        await database.connect()
        print("âœ… VeritabanÄ± baÄŸlantÄ±sÄ± kuruldu")
        
        # Veri yÃ¼kle ve kaliteyi analiz et
        await trainer.load_all_posts()
        trainer.analyze_data_quality()
        
        # Modeli eÄŸit
        await trainer.train_model()
        
        # PerformansÄ± deÄŸerlendir
        trainer.evaluate_clustering_performance()
        await trainer.evaluate_recommendation_quality()
        trainer.evaluate_tag_analysis()
        trainer.generate_topic_analysis()
        
        # RaporlarÄ± oluÅŸtur ve kaydet
        trainer.save_model_and_report()

        print("\nğŸ‰ MODEL EÄÄ°TÄ°MÄ° VE PERFORMANS ANALÄ°ZÄ° TAMAMLANDI!")
        print("=" * 60)
        print(f"ğŸ“Š Toplam iÅŸlenen post: {len(trainer.posts_data):,}")
        print(f"âš¡ EÄŸitim sÃ¼resi: {trainer.training_stats.get('training_time', 0):.2f} saniye")
        print("ğŸ“ SonuÃ§lar 'trained_models/' ve 'reports/' klasÃ¶rlerinde")

    except Exception as e:
        print(f"âŒ Hata oluÅŸtu: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if database.is_connected:
            await database.disconnect()
            print("âœ… VeritabanÄ± baÄŸlantÄ±sÄ± kapatÄ±ldÄ±")

if __name__ == "__main__":
    asyncio.run(main()) 